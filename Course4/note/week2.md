# Week 2

## 1. Case Studies

### a. Classic networks

+ LeNet-5
+ AlexNet
+ VGG

#### i. LeNet-5

<p align="center">
  <img src="../res/img/img18.png" width="600"/>
</p>

#### ii. AlexNet

<p align="center">
  <img src="../res/img/img19.png" width="600"/>
</p>

#### iii. VGG-16

<p align="center">
  <img src="../res/img/img20.png" width="600"/>
</p>

### b. ResNets

#### i. Residual block

<p align="center">
  <img src="../res/img/img21.png" width="600"/>
</p>

#### ii.Residual Network

"plain network"

<p align="center">
  <img src="../res/img/img22.png" width="600"/>
</p>

#### iii. Why ResNets work

Identity function is easy for Residual block to learn -> will not hurt the performance 

<p align="center">
  <img src="../res/img/img23.png" width="600"/>
</p>

#### iv. Image

<p align="center">
  <img src="../res/img/img24.png" width="600"/>
</p>

### c. 1X1 Convolutions

Network in network

<p align="center">
  <img src="../res/img/img25.png" width="600"/>
</p>

Shrink networks

<p align="center">
  <img src="../res/img/img26.png" width="600"/>
</p>

### d. Inception Network

#### i. Motivation

<p align="center">
  <img src="../res/img/img27.png" width="600"/>
</p>

+ The problem of computational cost
+ Different structure: using 1X1 convolution -> bottleneck layer

<p align="center">
  <img src="../res/img/img28.png" width="500"/>
  <img src="../res/img/img29.png" width="500"/>
</p>