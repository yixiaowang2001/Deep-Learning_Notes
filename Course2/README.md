# Improving Deep Neural Networks: Hpyerparameter Tuning, Regularization and Optimization
Coursera course from DeepLearning.AI ([link](https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning))

<div align="center">

| **Week** |                                              **Note**                                             |                                                 **Code**                                                |              **Status**              |                    **Keywords**                     |
|:--------:|:-------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|:------------------------------------:|:-------------------------------------------------------------------------------------------------------:|
|   week1  | [week1.md](https://github.com/yixiaowang2001/Deep-Learning_Notes/blob/main/Course2/note/week1.md) | [week1](https://github.com/yixiaowang2001/Deep-Learning_Notes/blob/main/Course2/code/week1) |  ![pb1](https://progress-bar.dev/100) | Bias/Variance, Regularization, Dropout |
|   week2  | [week2.md](https://github.com/yixiaowang2001/Deep-Learning_Notes/blob/main/Course2/note/week2.md) | [week2.ipynb](https://github.com/yixiaowang2001/Deep-Learning_Notes/blob/main/Course2/code/week2.ipynb) |  ![pb2](https://progress-bar.dev/100) | Mini-batch Gradient Descent, Exponentially Weighted Averages, Gradient Descent with Momentum, RMSprop, Adam, Learning Rate Decay |
|   week3  | [week3.md](https://github.com/yixiaowang2001/Deep-Learning_Notes/blob/main/Course2/note/week3.md) | [week3.ipynb](https://github.com/yixiaowang2001/Deep-Learning_Notes/blob/main/Course2/code/week3.ipynb) |  ![pb3](https://progress-bar.dev/100)  |  Hyperparameter Tuning, Batch Normalization, Multi-class, Framework(TensorFlow)  |
</div>