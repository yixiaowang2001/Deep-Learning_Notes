{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff930e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39db168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(4, 5)\n",
    "\n",
    "y = np.sum(x, axis=1,  keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07939887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    a = x + 1\n",
    "    b = x - 1\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e21d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = test(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dff4161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set-up variables\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    \n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        #(approx. 3 lines)\n",
    "        # theta_plus =                                        # Step 1\n",
    "        # theta_plus[i] =                                     # Step 2\n",
    "        # J_plus[i], _ =                                     # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i] += epsilon\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        #(approx. 3 lines)\n",
    "        # theta_minus =                                    # Step 1\n",
    "        # theta_minus[i] =                                 # Step 2        \n",
    "        # J_minus[i], _ =                                 # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i] -= epsilon\n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        # (approx. 1 line)\n",
    "        # gradapprox[i] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i]-J_minus[i]) / (2*epsilon)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    # (approx. 3 line)\n",
    "    # numerator =                                             # Step 1'\n",
    "    # denominator =                                           # Step 2'\n",
    "    # difference =                                            # Step 3'\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    numerator = np.linalg.norm(grad-gradapprox)                    \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator   \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
